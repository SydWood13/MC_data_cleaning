---
title: "Feedback Viewing"
author: "Sydney Wood"
output:
  html_document:
    df_print: paged
---

## Libraries and Packages
```{r}
# Package names
packages <- c("ggplot2",  "tidyverse", "apaTables", "colourpicker", "swirl", "dplyr", "gridExtra", "knitr", "lme4", "reshape2", "stargazer",  "did", "plm", "sandwich", "coefplot", "afex", "jtools", "glmmTMB", "psych", "gtsummary", "broom", "optimx", "simr")



# Install packages not yet installed
installed_packages <- packages %in% rownames(installed.packages())
if (any(installed_packages == FALSE)) {
  install.packages(packages[!installed_packages])
}

# Packages loading
invisible(lapply(packages, library, character.only = TRUE))
```



# Data cleaning

## exam files


done for each observation changing file names to correspond with file --- not the most efficient 

    Time 1 = Exam 1 feedback views & Exam 1 written score 
    Time 2 = Exam 2 feedback views & Exam 2 written score 
    Time 3 = Exam 3 feedback views & Exam 3 written score 
    Time 4 = Exam 4 feedback views & Exam 4 written score 
    No time 5 since they don't recieve feedback before the end of the quarter
    


time1data <- read.csv(file = "1_A_Feb15.csv")

time1data <- time1data %>% mutate(writtenScore = rowSums(across(14:31)))

time1data <- time1data %>% mutate(pwritten = writtenScore/120)
time1data <- time1data %>% filter(!is.na(View.Count))

time1data <- time1data %>% select("SID", "Total.Score", "View.Count", "pwritten")
time1data <- time1data %>% mutate(time = 2)

write.csv(time1data, file = "time2D.csv", row.names = FALSE)



## combining exams 


data2A <- read.csv(file = "time2A.csv")

data2B <- read.csv(file = "time2B.csv")

data2C <- read.csv(file = "time2C.csv")

data2D <- read.csv(file = "time2D.csv")


data2 <- rbind(data2A, data2B, data2C, data2D)

write.csv(data2, file = "FeedbackTime2.csv", row.names = FALSE)



## combining observations 


data1 <- read.csv("FeedbackTime1.csv")
data2 <- read.csv("FeedbackTime2.csv")
data3 <- read.csv("FeedbackTime3.csv")
data4 <- read.csv("FeedbackTime4.csv")

data <- rbind(data1, data2, data3, data4)

write.csv(data, file = "FeedbackFull.csv", row.names = FALSE)

# Descriptives 

## full data 


```{r}
data <- read.csv("FeedbackFull.csv")

data$time <- as.factor(data$time)

data <- data %>% mutate(viewed = case_when(View.Count == 0 ~ 0, View.Count > 0 ~ 1, .default = View.Count))

data <- data %>% mutate(Total.Score= Total.Score/150 *100) %>% mutate(pwritten = pwritten*100) %>% filter(opt_in != "#N/A")

data$student_id <- as.factor(data$SID)
data$SID <- as.factor(data$SID)
data$opt_in <- as.factor(data$opt_in)


length(unique(data$student_id))


```


```{r}

fulldata <- read.csv("~/GitHub/MC_data_cleaning/2023 data/Final_Data.csv")


score_data <- fulldata %>% select(SID = Real_id, anon_id = student_id, opt_in, exam1:exam5, p_tot1:p_tot5)



score_data_l <- gather(score_data, key = "exam", value = score, p_tot1:p_tot5)

shapiro.test(score_data_l$score)



score_data$SID <- as.factor(score_data$SID)
score_data$opt_in <- as.factor(score_data$opt_in)

data <- right_join(data, score_data, by = c("SID", "opt_in"))

data <- data %>% mutate(f_score = case_when(time == "1" ~ p_tot2, time == "2"~p_tot3, time == "3"~p_tot4, time == "4" ~ p_tot5))

data <- data %>% mutate(p_score = case_when(time == "1" ~ exam1, time == "2"~exam2, time == "3"~exam3, time == "4" ~ exam4))


```




# Descriptives 

```{r}

optin_table <- score_data_l %>% group_by(opt_in) %>% summarise(
  n = n(),
  mean = mean(score, na.rm = T),
  sd = sd(score, na.rm = T),
  median = median(score, na.rm = T),
  MAD = mad(score, na.rm = T),
)
optin_table


scoreTable <- data %>% group_by(time) %>% summarise(variable = "Exam Scores", n = n(), 
                                                    mean = mean(p_score, na.rm=T), 
                                                    sd = sd(p_score, na.rm = T), 
                                                    median = median(p_score, na.rm = T), 
                                                    MAD = mad(p_score, na.rm = T), 
                                                    Q1 = quantile(p_score, probs = .25, na.rm = T),
                                                    Q3 = quantile(p_score, probs = .75, na.rm = T),
                                                    IQR = IQR(p_score, na.rm = T),
                                                    min = min(p_score, na.rm = T),
                                                    max = max(p_score, na.rm = T))

scoreTable


written_optin_Table <- desc_l %>% group_by(Exam) %>% group_by(opt_in, .add = TRUE) %>%  summarise(variable = "Written Score", n = n(), 
                                                    mean = mean(score, na.rm=T), 
                                                    sd = sd(score, na.rm = T), 
                                                    median = median(score, na.rm = T), 
                                                    MAD = mad(score, na.rm = T), 
                                                    Q1 = quantile(score, probs = .25, na.rm = T),
                                                    Q3 = quantile(score, probs = .75, na.rm = T),
                                                    IQR = IQR(score, na.rm = T),
                                                    min = min(score, na.rm = T),
                                                    max = max(score, na.rm = T))

written_optin_Table



write.csv(written_optin_Table, file = "performance_intervention.csv")
write.csv(writtenTable, file = "performance_overall.csv")



feedbackTable <- data %>% group_by(time) %>% summarise(variable = "Feedback Viewing", n = n(), 
                                                    mean = mean(viewed, na.rm=T), 
                                                    sd = sd(viewed, na.rm = T))

feedbackTable


feedback_optin_Table <- data %>% group_by(time)  %>% group_by(opt_in, .add = TRUE) %>% summarise(variable = "Feedback Viewing", n = n(), 
                                                    mean = mean(viewed, na.rm=T), 
                                                    sd = sd(viewed, na.rm = T))

feedback_optin_Table


feedback <- data %>% summarise(
  pViewed = mean(viewed, na.rm = T),
  sdViewed = sd(viewed, na.rm = T)
)

write.csv(feedback_optin_Table, file = "feedback_intervention.csv")
write.csv(feedbackTable, file = "feedback_overall.csv")

overall <- data %>% summarise(
  pViewed = mean(viewed, na.rm = T),
  sdViewed = sd(viewed, na.rm = T), 
  optin = sum(opt_in), 

)


```

unit <- c("Exam 1", "Exam 2", "Exam 3", "Exam 4", "Exam 5")
mean <- c(mean(desc$p_tot1, na.rm = T), mean(desc$p_tot2, na.rm = T), mean(desc$p_tot3, na.rm = T), mean(desc$p_tot4, na.rm = T), mean(desc$p_tot5, na.rm = T))
sd <- c(sd(desc$p_tot1, na.rm = T), sd(desc$p_tot2, na.rm = T), sd(desc$p_tot3, na.rm = T), sd(desc$p_tot4, na.rm = T), sd(desc$p_tot5, na.rm = T))
median <- c(median(desc$p_tot1, na.rm = T), median(desc$p_tot2, na.rm = T), median(desc$p_tot3, na.rm = T), median(desc$p_tot4, na.rm = T), median(desc$p_tot5, na.rm = T))
MAD <- c(mad(desc$p_tot1, na.rm = T), mad(desc$p_tot2, na.rm = T), mad(desc$p_tot3, na.rm = T), mad(desc$p_tot4, na.rm = T), mad(desc$p_tot5, na.rm = T))

perf_descriptives <- cbind(unit, mean, sd, median, MAD)



# RQ 1  Analysis 

Exam score --> views on that exam 
score and feedback are on the same exam 

### 

```{r}

rq1data <- data %>% select(p_score, viewed, time)




```

## Model 1 
```{r}

below50 <- rq1data %>% filter(p_score < 50)

59/14556

lin <- "viewed~p_score + (1|time) "
rq1mod1 <- glmer(lin, rq1data, family = "binomial")

#?glm

summ(rq1mod1)
sum1 <- summary(rq1mod1)
sum1
rq1data$linfit <- predict(rq1mod1, rq1data)

rq1data$linprobs <- exp(rq1data$linfit)/(1+exp(rq1data$linfit))

plot(rq1mod1)





```
MODEL FIT:
AIC = 1700.23, BIC = 1716.08
Pseudo-R² (fixed effects) = 0.05
Pseudo-R² (total) = 0.08 
DF = 1453

Fit with maximum likelihood (Laplace Approximation)

```{r}
exp(sum1$coefficients[2])
exp(sum1$coefficients[1])/(1+exp(sum1$coefficients[1]))

x = 100

o = sum1$coefficients[1] + (sum1$coefficients[2]*x) 


exp(o)/(1+exp(o))
```

According to the linear model, a person with a hypothetical 0% score on the exam would have a 14.94% probability of viewing their exam feedback. The model also showed a significant positive linear relationship. Each point increase of written score is a .03 logit increase or 1.03 times the probability of viewing feedback. A student with a score of 1% would be expected to have 15.37% probability. A student with the score of 50% would be expected have q 47.86% probability and a student with a score of 100% would be expected to have a 82.75% probability of viewing their feedback. 

Time ICC = .03 - 3% of the variance is due to differences across time quite small suggesting that time varying effects are likely insignificant. 


## Model 2
```{r}

rq1data <- rq1data %>% mutate(squared = (p_score^2)/100)

sqEq <- "viewed~p_score + squared + (1|time)"
rq1mod2 <- glmer(sqEq, rq1data, family = "binomial")

summ(rq1mod2)
sum2 <- summary(rq1mod2)
sum2
rq1data$sqfit <- predict(rq1mod2, rq1data)

rq1data$sqprobs <- exp(rq1data$sqfit)/(1+exp(rq1data$sqfit))

plot(rq1mod2)


anova(rq1mod1, rq1mod2)

```
MODEL FIT:
AIC = 1683.87, BIC = 1705.00
Pseudo-R² (fixed effects) = 0.08
Pseudo-R² (total) = 0.11 
DF = 1452

maximum likelihood (Laplace Approximation) [glmerMod]

```{r}


exp(sum2$coefficients[2])
exp(sum2$coefficients[1])/(1+exp(sum2$coefficients[1]))

x = 100

o = sum2$coefficients[1] + (sum2$coefficients[2]*x) + (sum2$coefficients[3]*(x^2/100))


exp(o)/(1+exp(o))

```


Significant quadratic relationship --
According to the model, a person with a hypothetical 0% score on the exam would have a 98.02% probability of viewing their exam feedback. The model also showed a significant positive linear slope of -.13. Each point increase of written score is a .12 logit decrease or .88 times the probability of viewing feedback. But the squared term is also signficant with a logit of 0.0011 or 1.0011 times the likelihood which is small but sugnificant at the .001 level. 

This means that, while accounting for time varying effects, a student with a hypothetical score of 1% would have a 97.76% probability of viewing their exam, but a student with a 50% score would have an expected probability of 56.09 % and a student with 100% on the exam would be expected to have a 89.84% probability of viewing thier feedback. 



Fits significantly better difference in X-square 18.361  p < .001 

## Model 3 --- failed to converge
```{r}

rq1data <- rq1data %>% mutate(cubed = (p_score^3/ 1000))

cubeEq <- "viewed~p_score + squared + cubed + (1|time)"
rq1mod3 <- glmer(cubeEq, rq1data, family = "binomial")
#remove(rq1mod3)
summ(rq1mod3)
summary(rq1mod3)

rq1data$cubefit <- predict(rq1mod3, rq1data)

rq1data$cubeprobs <- exp(rq1data$cubefit)/(1+exp(rq1data$cubefit))

plot(rq1mod3)

exp(-.01)

anova(rq1mod2, rq1mod3)
```

AIC = 1683.80, BIC = 1710.22
Pseudo-R² (fixed effects) = 0.08
Pseudo-R² (total) = 0.11 



## plot 

```{r}

colors <- c( "#FFFFFF", "#030303", "#A3A3A3", "#F5F5F5", "#191970", "#104E8B", "#00688B", "#00868B","#458B74", "#006400")

#colourPicker()

plot <-  ggplot(rq1data) +theme (axis.text.x = element_text(face = "bold", color = colors[2], 
                             size =15),
  axis.text.y = element_text(face = "bold", color = colors[2], 
                             size = 15), 
  legend.text = element_text(color = colors[2], face = "bold", size = 18), 
  legend.title = element_text(color = colors[2], face = "bold"),
  axis.title.x = element_text(color = colors[2],face = "bold",size = 20 ), 
  axis.title.y = element_text(color = colors[2],face = "bold",size = 20 ))

rq1Linplot <- plot + geom_point(aes(x = p_score, y = viewed), alpha = .05) + geom_smooth(aes(x = p_score, y = linprobs), method = "glm", 
    method.args = list(family = "binomial"), 
    se = FALSE) 

rq1Linplot

rq1Sqplot <- plot + geom_point(aes(x = p_score, y = viewed), alpha = .05) + geom_smooth(aes(x = p_score, y = sqprobs, color = time), method = "glm", method.args = list(family = "binomial"),  formula = y~ x + poly(x,2), linewidth = 1) + scale_x_continuous(name = "Performance on Exam") + scale_y_continuous(name = "Probability of Viewing Feedback") + scale_color_manual(values = c("#191970" ,"#436EEE", "#8968CD" ,"#4EEE94"))
rq1Sqplot
?geom_point
ggsave(rq1Sqplot, file = "Rq1Plot.png")

plot(rq1data$p_score, rq1data$sqprobs)

rq1Cubeplot <- plot + geom_point(aes(x = p_score, y = viewed), position = "jitter", linewidth = 1) + geom_smooth(aes(x = p_score, y = cubeprobs), method = "glm", method.args = list(family = "binomial"),  formula = y~ x + poly(x, 2) + poly(x,3)) 
?geom_smooth
rq1Cubeplot

```


## Exploratory 

```{r}
erq1 <- data %>% mutate(grade = case_when(p_score >= 90 ~ "A", p_score >= 80 ~ "B", p_score >= 70 ~ "C", p_score >= 60 ~ "D", p_score < 60 ~ "F"))

grade_viewing <- erq1 %>% group_by(grade) %>% summarise(
  proportion = mean(viewed, na.rm = T), sd = sd(viewed, na.rm = T)
)

grade_viewing

mod4 <- glm("viewed~grade", erq1, family = "binomial")

summ(mod4)



```


# RQ 2 Analyis 


### Visualize Feedback Viewing

```{r}

colors <- c("black", "#ACBFE6", "#465573", "#232A3A" )


plot <- ggplot(data) +theme (axis.text.x = element_text(face = "bold", color = colors[4], 
                             size =15),
  axis.text.y = element_text(face = "bold", color = colors[4], 
                             size = 15), 
  legend.text = element_text(color = colors[4], face = "bold", size = 18), 
  legend.title = element_text(color = colors[4], face = "bold"),
  axis.title.x = element_text(color = colors[4],face = "bold",size = 20 ), 
  axis.title.y = element_text(color = colors[4],face = "bold",size = 20 ))




view_optin <- plot + geom_bar(aes(x = viewed, y = after_stat(count/sum(count)), fill = opt_in), position = "dodge") + scale_fill_manual(values = c("#ACBFE6", "#465573"), name = " ", labels = c("Opt-out", "Opt-in")) + labs(x = "View Count", y = "Proportion") 

view_optin


ggsave(view_optin, file = "view_optin.png", height = 5, width = 11.5)
```


view = b0i * time1 + b1 * time2 + b2 * time3 + b4 * time4 + e 

b0j = b0 + studenti 

view = 1.81 + -.74 (if time2) + .58 (if time3) + -.76 (if time4)
### Model 1: Does Observation time predict Feedback Viewing? 

```{r}

model1 <- glmer(viewed ~  time + (1|student_id),  data, family = binomial)

summ(model1)
sum21 <- summary(model1)

sum21



view_time <- plot + geom_bar(aes(x = time, y = (viewed/370)*100, fill = opt_in), stat = "identity") + scale_y_continuous(name = "% Viewed") + scale_fill_manual(values = c(colors[2], colors[3]))


view_time



```
```{r}
exp(sum1$coefficients[2])
exp(sum21$coefficients[1])/(1+exp(sum21$coefficients[1]))



o = sum21$coefficients[1] + (sum21$coefficients[4])


exp(o)/(1+exp(o))
```
maximum likelihood (Laplace Approximation) [glmerMod]

Time 1 is between exam 1 & 2 
Time 2 is Between 2 & 3 
Time 3 is between 3 & 4 --- intervention time period
Time 4 is between 4 & 5 -- post intervention lag 1 

Pre-registered alpha of .01: 

significant decrease of feedback viewing between exam 2 & 3 (time2) compared to between Exam 1 & 2(time1). Significant increase in feedback viewing between exam 3 and exam 4 compared to time 1. Significant decrease in feedback viewing between 4 & 5 compared to time1. 





Student who opt-in are overall more likely to view their feedback. Not surprising since students are forced to look at feedback to do this intervention. 


viewed = b0i*time1 + b1*optin + b2*time2 + b3*time3 + b4*time4 + b5*optin*time2 + b6*time3*optin + b7*time4*optin + e 

b0i = b0 + studenti


### Model 2: Feedback viewing over time by intervention group 
```{r}

eq2 <- viewed ~ opt_in + time + opt_in*time + (1|student_id)
model2 <- glmer(eq2, data = data,  family = binomial)

summ(model2)

sum22 <- summary(model2)
sum22

anova(model1, model2)

```

```{r}
#exp(sum22$coefficients[1])/(1+exp(sum22$coefficients[1]))

#exp(sum22$coefficients[2])
#exp( sum22$coefficients[2] + sum22$coefficients[7] )

time2 = 0
time3 = 1
time4 = 0
optin = 1

o = sum22$coefficients[1] + sum22$coefficients[3]*time2 + sum22$coefficients[4]*time3 + sum22$coefficients[5]*time4  + sum22$coefficients[2]*optin + sum22$coefficients[6]*optin*time2 + sum22$coefficients[7]*optin*time3 + sum22$coefficients[8]*optin*time4


exp(o)/(1+exp(o))
```




Correlational results: not a difference in difference 
Students who opted out f the intervention had a marginally significant decrease in viewing behavior between exam 2 & 3 compared to time1 (between exam 1 & 2), but otherwise their feedback viewing behavior did not change over time. Student's who opted into the intervention only had a significant increase in viewing behavior after the intervention! 



AIC = 1473.51, BIC = 1521.06
Pseudo-R² (fixed effects) = 0.14
Pseudo-R² (total) = 0.62 





The only time that feedback viewing behavior significantly differed by group is between Exam 3 & 4 when the students who did the intervention were required to look at and correct thier feedback. 




# RQ 3: Performance 

## Descriptives 

```{r}
data$viewed <- as.factor(data$viewed)
opt_in_data <- data %>% filter(opt_in == 1)

opt_out_data <- data %>% filter(opt_in == 0)

performance_in_feedback <- opt_in_data %>% group_by(time) %>%  group_by(viewed, .add = T) %>% summarise(variable = "Subsequent Score", n = n(), 
                                                    mean = mean(f_score, na.rm=T), 
                                                    sd = sd(f_score, na.rm = T), 
                                                    median = median(f_score, na.rm = T), 
                                                    MAD = mad(f_score, na.rm = T), 
                                                    Q1 = quantile(f_score, probs = .25, na.rm = T),
                                                    Q3 = quantile(f_score, probs = .75, na.rm = T),
                                                    IQR = IQR(f_score, na.rm = T),
                                                    min = min(f_score, na.rm = T),
                                                    max = max(f_score, na.rm = T))
  
performance_in_feedback

performance_out_feedback <- opt_out_data %>% group_by(time) %>%  group_by(viewed, .add = T) %>% summarise(variable = "Subsequent Score", n = n(), 
                                                    mean = mean(f_score, na.rm=T), 
                                                    sd = sd(f_score, na.rm = T), 
                                                    median = median(f_score, na.rm = T), 
                                                    MAD = mad(f_score, na.rm = T), 
                                                    Q1 = quantile(f_score, probs = .25, na.rm = T),
                                                    Q3 = quantile(f_score, probs = .75, na.rm = T),
                                                    IQR = IQR(f_score, na.rm = T),
                                                    min = min(f_score, na.rm = T),
                                                    max = max(f_score, na.rm = T))
  
performance_out_feedback


write.csv(performance_in_feedback, file = "Opt-in Performance over time by feedback viewing.csv")

write.csv(performance_out_feedback, file = "Opt-out Performance over time by feedback viewing.csv")


performance_feedback <- data %>% group_by(time)  %>% summarise(variable = "Subsequent Score", n = n(), 
                                                    mean = mean(f_score, na.rm=T), 
                                                    sd = sd(f_score, na.rm = T), 
                                                    median = median(f_score, na.rm = T), 
                                                    MAD = mad(f_score, na.rm = T), 
                                                    Q1 = quantile(f_score, probs = .25, na.rm = T),
                                                    Q3 = quantile(f_score, probs = .75, na.rm = T),
                                                    IQR = IQR(f_score, na.rm = T),
                                                    min = min(f_score, na.rm = T),
                                                    max = max(f_score, na.rm = T))
  
performance_feedback

write.csv(performance_feedback, file = "overall performance over time by feedback viewing.csv")

data %>% group_by()


```



## Model 1: Does feedback viewing on Exam X predict performance on Exam X+1?
```{r}

eq1 <- f_score ~ viewed + (1|student_id) + (1|time)

m1 <- lmer(eq1, data)

summary(m1)


```
 REML. t-tests use Satterthwaite's method [lmerModLmerTest]

Yes, when accounting for with-in individual variances and shared variances for each exam, students who viewed their feedback scored higher (M= 79.44, SD = 14.55)  than those who did not (M = 70.81, SD = 14.56), t = 4.94 p < .01 



## Is feedback viewing sufficient to serve as the mechanism that explains the changes in performance trajectories for the two groups before and after the intervention?


## Model 2 

### Performance over time 
```{r}
eq2 <- f_score ~ time + (1|student_id)

m2 <- lmer(eq2, data)

summ(m2)  
  
```

Overall, students perform better over time, p < .01. 


### Performance over time by feedback viewing  
```{r}
eq3 <- f_score ~ time + viewed + viewed*time + (1|student_id) 

m3 <- lmer(eq3, data)

summ(m3)  
  



```

Overall, students who viewed thier feedback did better than those who didn't, but the effect of feedback viewing did change based on time point. 


### Performance over time by intervention and feedback viewing 
```{r}
eq4 <- f_score ~ time + viewed + opt_in  + time*opt_in + time*viewed + opt_in*viewed + time*viewed*opt_in + (1|student_id)

m4 <- lmer(eq4, data)

sum32 <- summary(m4)  

```
Linear mixed model fit by REML. t-tests use Satterthwaite's method [
lmerModLmerTest]

Even when controlling for group and feedback viewing students tend to better over time. When controlling for group and time, students who view feedback did better than those who did not. Surprisingly, when controlling for time and feedback viewing, students who opted-in did not do better than students who opted out. There were no differences at the two-way and three-way interactions. This is very surprising and does suggest that increased feedback viewing is a plausible explanation for the difference in performance seen in Wood & Cross (2024). 

## Estimated Performance

```{r}

time2 = 0
time3 = 0
time4 = 1
optin = 1
views = 1 




sum32$coefficients[1] + sum32$coefficients[2]*time2 + sum32$coefficients[3]*time3 + sum32$coefficients[4]*time4 + sum32$coefficients[5]*views + sum32$coefficients[6]*optin + sum32$coefficients[7]*time2*optin + sum32$coefficients[8]*time3*optin + sum32$coefficients[9]*time4*optin + sum32$coefficients[10]*time2*views + sum32$coefficients[11]*time3*views + sum32$coefficients[12]*time4*views + sum32$coefficients[13]*views*optin +  sum32$coefficients[14]*time2*views*optin + sum32$coefficients[15]*time3*views*optin + sum32$coefficients[16]*time4*views*optin




```


# RQ4 
```{r}
mdata <- read.csv("~/GitHub/MC_data_cleaning/W23 Rubric Item Data/finalMistakesData.csv")

mistakes <- read.csv("~/GitHub/MC_data_cleaning/W23 Rubric Item Data/mistakes.csv") %>% select(-X) %>% mutate(time = exam -1 )

```


### Possible items

H0 = 9 --- only 3 effectively since some are mutually exclusive  
H1 = 7 --- only 5 effectively since some are mutually exclusive
OPDEF = 27 --- only 15 effectively since some are mutually exclusive  
Interpret = 6 --- only 3 effectively since some are mutually exclusive
COMPCV = 17 --- only 9 effectively since some are mutually exclusive
CV = 31 --- only 15 effectively since some are mutually exclusive
CONFOUND = 14 --- only 4 effectively since some are mutually exclusive
IVTHREAT = 13 --- only 5 effectively since some are mutually exclusive


## Descriptives
```{r}




mstudent <- mdata %>% filter(!is.na(pRep)) %>% 
  group_by(student_id, time, attempt, question_id) %>% 
  summarise(
    nRep = sum(numRep, na.rm = T), 
    nPast = sum(pastMistakes, na.rm = T),
    numMistakes = sum(currentMistakes, na.rm = T),
    pRep = mean(pRep)
  ) 

mean(mstudent$numMistakes)

mobs <- mstudent %>% filter(nRep > 0)

717/3547



length(unique(mstudent$student_id))

nstudent <- mstudent %>% group_by(student_id) %>% summarise(
  nRep = sum(nRep), 
  nPast = sum(nPast), 
  nCurrent= sum(numMistakes),
  avgRep = mean(pRep),
  sdRep = sd(pRep), 
  pRep = nRep/nPast*100
) 

length(unique(mstudent$student_id))

(sum(nstudent$nRep))/369
(sum(nstudent$nCurrent))/369

sd(nstudent$nCurrent)
sd(nstudent$nRep)

nmstudent <- nstudent%>% filter(nRep>0)



(sum(nmstudent$nRep))/314



314/369 - 1

mboth <- mstudent %>% group_by(student_id, time, attempt) %>% summarise(
    numRep = sum(nRep),
  numMis = sum(numMistakes),
  d = sum(nPast), 
  pRep = numRep/d*100
)
mtime <- mstudent %>% group_by(student_id, time) %>% summarise(
    numRep = sum(nRep),
  numMis = sum(numMistakes),
    d = sum(nPast), 
  pRep = numRep/d*100
)
mattempt <- mstudent %>% group_by(student_id, attempt) %>% summarise(
    numRep = sum(nRep),
  numMis = sum(numMistakes),
    d = sum(nPast), 
  pRep = numRep/d*100
  
)


nonzero <- mboth %>% filter(numRep != 0)

mTime <- mtime %>% group_by(time) %>% 
  summarise( attempt = "none",
             n = n(), 
             totalRep = sum(numRep),
             mean = mean(numRep, na.rm = T),
             sd = sd(numRep, na.rm = T),
             median = median(numRep, na.rm = T),
             MAD = mad(numRep, na.rm = T)
  )

mAttempt <- mattempt %>% group_by(attempt) %>% 
  summarise( time = "none",
             n = n(), 
             totalRep = sum(numRep),
             mean = mean(numRep, na.rm = T),
             sd = sd(numRep, na.rm = T),
             median = median(numRep, na.rm = T),
             MAD = mad(numRep, na.rm = T)
  ) %>% select(time, attempt, n, totalRep, mean, sd, median, MAD)

mQ <- mboth %>% group_by(time, attempt) %>% 
  summarise(
            n = n(), 
             totalRep = sum(numRep, na.rm = T),
             mean = mean(numRep, na.rm = T),
             sd = sd(numRep, na.rm = T),
             median = median(numRep, na.rm = T),
             MAD = mad(numRep, na.rm = T)
  )

mdesc <- rbind(mTime, mAttempt, mQ)

write.csv(mdesc, "mistakeDescriptives.csv", row.names = F)

mTimeP <- mtime %>% group_by(time) %>% 
  summarise( attempt = "none",
             n = n(), 
             totalRep = sum(numRep),
             mean = mean(pRep, na.rm = T),
             sd = sd(pRep, na.rm = T),
             median = median(pRep, na.rm = T),
             MAD = mad(pRep, na.rm = T)
  )

mAttemptP <- mattempt %>% group_by(attempt) %>% 
  summarise( time = "none",
             n = n(), 
             totalRep = sum(numRep),
             mean = mean(pRep, na.rm = T),
             sd = sd(pRep, na.rm = T),
             median = median(pRep, na.rm = T),
             MAD = mad(pRep, na.rm = T)
  ) %>% select(time, attempt, n, totalRep, mean, sd, median, MAD)

mQp <- mboth %>% group_by(time, attempt) %>% 
  summarise(
            n = n(), 
             totalRep = sum(numRep, na.rm = T),
             mean = mean(pRep, na.rm = T),
             sd = sd(pRep, na.rm = T),
             median = median(pRep, na.rm = T),
             MAD = mad(pRep, na.rm = T)
  )

mdescP <- rbind(mTimeP, mAttemptP, mQp)

write.csv(mdescP, "%mistakeDescriptives.csv", row.names = F)

shapiro.test(nonzero$pRep)




nzTime <- nonzero %>% group_by(time) %>% 
  summarise( attempt = "none",
             n = n(), 
             totalRep = sum(numRep),
             mean = mean(pRep, na.rm = T),
             sd = sd(pRep, na.rm = T),
             median = median(pRep, na.rm = T),
             MAD = mad(pRep, na.rm = T)
  )

nzAttempt <- nonzero %>% group_by(attempt) %>% 
  summarise( time = "none",
             n = n(), 
             totalRep = sum(numRep),
             mean = mean(pRep, na.rm = T),
             sd = sd(pRep, na.rm = T),
             median = median(pRep, na.rm = T),
             MAD = mad(pRep, na.rm = T)
  ) %>% select(time, attempt, n, totalRep, mean, sd, median, MAD)

nzQ <- nonzero %>% group_by(time, attempt) %>% 
  summarise(
            n = n(), 
             totalRep = sum(numRep),
             mean = mean(pRep, na.rm = T),
             sd = sd(pRep, na.rm = T),
             median = median(pRep, na.rm = T),
             MAD = mad(pRep, na.rm = T)
  )

nzdesc <- rbind(nzTime, nzAttempt, nzQ)

write.csv(mdesc, "non_zero_mistakeDescriptives.csv", row.names = F)

```
### Plot 
```{r}
colors <- c("black", "#ACBFE6", "#465573", "#232A3A","#8AC1FF")

#colourpicker::colourPicker()
plot1 <- ggplot(nstudent) +theme(axis.text.x = element_text(face = "bold", color = colors[4], 
                             size =20, family = "serif"),
   plot.title = element_text(face = "bold", color = colors[4], 
                             size = 35, family = "serif"),                          
  axis.text.y = element_text(face = "bold", color = colors[4], 
                             size = 20),
  legend.text = element_text(color = colors[4], face = "bold", size = 20), 
  legend.title = element_text(color = colors[4], face = "bold"),
  axis.title.x = element_text(color = colors[4],face = "bold",size = 25 ), 
  axis.title.y = element_text(color = colors[4],face = "bold",size = 25 ))



plotStudents <- plot1 + geom_histogram(aes(x = pRep, y = ((..count..)/sum(..count..))), fill = colors[5]) +labs(title = "Distribution of Percentage of Mistakes Repeated" ) +xlab("Percent Repeated") + ylab("Proportion of Students")

plotStudents

ggsave(plotStudents, file = "repDist.png", height = 8.2, width = 13.6)


plot2 <- ggplot(mstudent) +theme(axis.text.x = element_text(face = "bold", color = colors[4], 
                             size =20, family = "serif"),
   plot.title = element_text(face = "bold", color = colors[4], 
                             size = 35, family = "serif"),                          
  axis.text.y = element_text(face = "bold", color = colors[4], 
                             size = 20),
  legend.text = element_text(color = colors[4], face = "bold", size = 20), 
  legend.title = element_text(color = colors[4], face = "bold"),
  axis.title.x = element_text(color = colors[4],face = "bold",size = 25 ), 
  axis.title.y = element_text(color = colors[4],face = "bold",size = 25 ))

plotObservations <- plot2 + geom_histogram(aes(x = pRep, y = ((..count..)/sum(..count..)), fill = question_id)) +labs(title = "Distribution of Percentage of Mistakes Repeated" ) +xlab("Percent Repeated") + ylab("Proportion of Observations")

plotObservations

ggsave(plotObservations, file = "ObSrepDist.png", height = 8.2, width = 13.6)


```

## Model 1 

### Poisson 
```{r}

mstudent$time <- as.factor(mstudent$time)
mstudent$attempt <- as.factor(mstudent$attempt)
mstudent$student_id <- as.factor(mstudent$student_id)

rq4eq1c <- "nRep ~ attempt + (1|question_id) +  (1|student_id) "

rq4mc <- glmer(rq4eq1c, mstudent, family = "poisson")

summ(rq4mc)
sum41c <- summary(rq4mc)



```

MODEL INFO:
Observations: 3634
Dependent Variable: nRep
Type: Mixed effects generalized linear regression
Error Distribution: poisson
Link function: log 


```{r}


tbl_regression(rq4m, exponentiate = T)

exp(sum41$coefficients[1])/(1+exp(sum41$coefficients[1]))


```
#### INTERPRETATION: 

Students odds of making a repeated mistake on their 3rd time responding to the prompt significantly descreased, however for the questions that were attempt 3 or 4 times prior they were significantly less likely to repeat thier mistakes. 


### Percents 
```{r}


rq4eq1p <- "pRep ~ attempt + (1|question_id) +  (1|student_id) "

rq4mp <- lmer(rq4eq1p, mstudent)

summ(rq4mp)
sum41p <- summary(rq4mp)



```
### Interpretation

The percentage of repeated mistakes significantly decrease after each attempt. 

On Exam 2, the expected average of repeated mistakes was 7.93%. due to inflated zeros in the data the expect decrease in the % of repeated mistakes actually goes negative for attempt 3 & 4. This suggests an ill fitting model due to distribution of % repeated mistakes.  




#Model 2 


### Poisson 
```{r}

grouping <- data %>% select(student_id, time, viewed)
opt_in <- data %>% select(student_id, opt_in)

dat <- left_join(mstudent, grouping, by = c("student_id", "time")) %>% mutate(viewed = case_when(is.na(viewed)~ 0, .default = viewed))

dat <- left_join(dat, opt_in, by = c("student_id"))
dat$viewed <- as.factor(dat$viewed)



rq4eq2c <- "nRep ~ time + opt_in + viewed + time*opt_in * time*viewed + viewed*opt_in + time*opt_in*viewed + (1|student_id) + (1|question_id) + (1|attempt)"

rq4m2c <- glmer(rq4eq2c, dat, family = "poisson",  control = glmerControl(
                           optimizer="bobyqa",
                                 optCtrl=list(maxfun=2e5)))

summ(rq4m2c)
sum42c <- summary(rq4m2c)



```

#### Interpreatation
Overall students make fewer repeated mistakes over time, but this effect is not different for the different groups. We do see an increase in repeated mistakes between exam 3 and 4 for students who viewed there feedback. This is directly counter expected and counter to the aims of the intervention. 




```{r}

tbl_regression(rq4m2, exponentiate = T)




```



### Percents 
```{r}



rq4eq2p <- "pRep ~ time + opt_in + viewed + time*opt_in * time*viewed + viewed*opt_in + time*opt_in*viewed + (1|student_id) + (1|question_id) + (1|attempt)"

rq4m2p <- lmer(rq4eq2p, dat,  control = lmerControl(
                           optimizer="bobyqa",
                                 optCtrl=list(maxfun=2e5)))

summ(rq4m2p)
sum42p <- summary(rq4m2p)

sum42p

```


